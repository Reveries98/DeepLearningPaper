%!TEX root = ../main_article.tex

\begin{abstract}
    Code search is a foundational task in software development, which studies semantic similarity between natural language queries and program code. Recent years have witnessed great progress made in code search. However, researchers tend to pretrain a code representation model in different program languages, so that it can be used in code related downstream problems, and then fine-tune it in code search with a specific program language. Our empirical study shows that there may be performance damage when fine-tuning code search in multiple program languages. We believed that it is caused by the disentanglement between code identifier information and code sentiment information. Therefore, we proposed a search enhancement framework based on adversarial learning, so as to reduce identifier information provided by pretrained models.
\end{abstract}